set.seed(1)
sample(10)
RNGkind(sample.kind = "Rounding")
sample(10) # 9  4  7  1  2  5  3 10  6  8
sample(10) # 9  4  7  1  2  5  3 10  6  8
set.seed(1)
sample(10) # 9  4  7  1  2  5  3 10  6  8
# Question 1
library(ISLR)
# Question 1
library(ISLR)
names(Hitters)
### 6.5.1 Best Subset Selection ###
library(ISLR)
RNGkind(sample.kind = "Rounding")
set.seed(1)
sample(10) # 3  4  5  7  2  8  9  6 10  1
# Question 1
library(ILSR)
# Question 1
library(ISLR)
### 6.5.1 Best Subset Selection ###
library(ISLR)
library(leaps)
install.packages("ISLR")
# Question 1
library(ISLR)
names(Hitterz)
names(Hitter)
install.packages("leaps")
names(Hitter)
# Question 1
library(ISLR)
names(Hitter)
fix(Hitter)
names(Hitter)
View(Hitter)
remove(Hitter)
names(Hitters)
dim(Hitters)
# remove nulls
Hitters <- na.omit(Hitters)
dim(Hitters)
# train the model
regfit.full <- regsubsets(Salary~., Hitters, nvmax=19)
# train the model
library(leaps)
regfit.full <- regsubsets(Salary~., Hitters, nvmax=19)
coef(regfit.full, 7)
coef(regfit.full, 8)
### Question 2
reg.summary <- summary(regfit.full)
plot(reg.summary$adjr2, xlab='Number of Variables', ylab='Adjusted RSq', type='l')
which.max(reg.summary$adjr2)
reg.summary$adjr2
reg.summary$adjr2[11]
reg.summary$adjr2
plot(reg.summary$bic, xlab='Number of Variables', ylab='BIC', type='l')
which.min(reg.summary$bic) # 11
reg.summary$bic[6] # 0.5225706
reg.summary$bic
install.packages("glmnet")
### Question 4
library(glmnet)
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
plot(ridge.mod)
ridge.mod
ridge.mod$lambda
dim(ridge.mod)
dim(ridge.mod$lambda)
ridge.mod$lambda
ridge.mod$coef
coef(ridge.mod)[,0]
coef(ridge.mod)[,1]
coef(ridge.mod)[,100]
coef(ridge.mod)[,101]
coef(ridge.mod)[,100]
coef(ridge.mod)[,20]
coef(ridge.mod)[,50]
coef(ridge.mod)[16,100] # DivisionW
### Question 5
set.seed(1)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2) # the vector of numbers
test <- (-train)
y.test <- y[test]
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2) # 100743.4
out <- glmnet(x, y, alpha = 1,lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0]) - 1
install.packages("pls")
### Question 7
library(pls)
set.seed(2)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type='MSEP')
pcr.pred <- predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred - y.test)^2)
pcr.fit <- pcr(y~x, scale=TRUE, ncomp=7)
summary(pcr.fit)
summary(pcr.fit)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type='MSEP')
summary(pcr.fit)
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2) # the vector of numbers
test <- (-train)
y.test <- y[test]
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam # 35.32063
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2) # 100743.4
out <- glmnet(x, y, alpha = 1,lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef # 6
length(lasso.coef[lasso.coef!=0]) - 1
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2) # the vector of numbers
test <- (-train)
y.test <- y[test]
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam # 16.78016
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2) # 100743.4
out <- glmnet(x, y, alpha = 1,lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef # 7
length(lasso.coef[lasso.coef!=0]) - 1
bestlam # 16.78016
### Question 4
library(glmnet)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
### Question 1
# import
library(ISLR)
names(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
plot(ridge.mod)
# remove nulls
Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., Hitters)[, -1]
y <- Hitters$Salary
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x, y, alpha=0, lambda=grid)
plot(ridge.mod)
coef(ridge.mod)[,100] # DivisionW
library(ISLR)
attach(Wage)
### 7.8.1 Polynomial Regression and Step Functions ###
fit <- lm(wage~poly(age, 4), data=Wage)
coef(summary(fit))
fit2 <- lm(wage~poly(age, 4, raw=T), data=Wage)
coef(summary(fit2))
fit2a <- lm(wage~age+I(age^2)+I(age^3)+I(age^4), data=Wage)
coef(fit2a)
fit2b <- lm(wage~cbind(age, age^2, age^3, age^4), data=Wage)
coef(fit2b)
# prediction
agelims <- range(age)
agelims
age.grid <- seq(from=agelims[1], to=agelims[2])
age.grid
preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)
preds
se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands
# plot
par(mfrow(c(1,2), mar=c(4.5, 4.5, 1, 1), oma=c(0,0,4,0)))
# plot
par(mfrow=(c(1,2), mar=c(4.5, 4.5, 1, 1), oma=c(0,0,4,0)))
# plot
par(mfrow=c(1,2), mar=c(4.5, 4.5, 1, 1), oma=c(0,0,4,0)))
# plot
par(mfrow=c(1,2), mar=c(4.5, 4.5, 1, 1), oma=c(0,0,4,0))
plot(age, wage, xlim=agelims, cex=.5, col='darkgrey')
title('Degree-4 Polynomial', outer=T)
lines(age.grid, preds$fit, lwd=2, col='blue')
matlines(age.grid, se.bands, lwd=1, col='blue', lty=3)
preds2 <- predict(fit2, newdata=list(age=age.gridla, se=TRUE))
preds2 <- predict(fit2, newdata=list(age=age.grid, se=TRUE))
max(abs(preds$fit - preds2$fit))
max(abs(preds$fit))
preds2 <- predict(fit2, newdata=list(age=age.grid), se=TRUE))
preds2 <- predict(fit2, newdata=list(age=age.grid), se=TRUE)
max(abs(preds$fit - preds2$fit))
# Decide the degree by ANOVA
fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~age+poly(age, 2), data=Wage)
fit.2 <- lm(wage~age+poly(age, 2)+poly(age, 3), data=Wage)
fit.2 <- lm(wage~age+poly(age, 2), data=Wage)
fit.3 <- lm(wage~age+poly(age, 3), data=Wage)
fit.4 <- lm(wage~age+poly(age, 4), data=Wage)
fit.5 <- lm(wage~age+poly(age, 5), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# obtain p-values
coef(summary(fit.5))
# the square of the t-statistics are equal to the F-statistics
(-11.983)^2
# ANOVA works when we have other terms
fit.1 <- lm(wage~education+age, data=Wage)
fit.2 <- lm(wage~education+poly(age, 2), data=Wage)
fit.3 <- lm(wage~education+poly(age, 3), data=Wage)
anova(fit.1, fit.2, fit.3)
# predict whether an individual earns more than $250,000
fit <- glm(I(wage > 250)~poly(age, 4), data=Wage, family=binomial)
preds <- predict(fit, newdata=list(age=age.grid), se=T)
# calculate the confidence intervals
pfit <- exp(preds$fit) / (1 + exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands <- exp(preds$se.bands.logit) / (1 + exp(preds$se.bands.logit))
se.bands <- exp(se.bands.logit) / (1 + exp(se.bands.logit))
se.bands
preds
# directly compute the probabilities
preds <- predict(fit, newdata=list(age=age.grid), type='response', se=T)
# plot
plot(age, I(wage > 250), xlim=agelims, type='n', ylim=c(0, .2))
points(jitter(age), I((wage > 250) / 5), cex=.5, pch='|')
lines(age.grid, pfit, lwd=2, col='blue')
matlines(age.grid, se.bands, lwd=1, col='blue', lty=3)
# step function
table(cut(age, 4))
fit <- lm(wage~cut(age, 4), data=Wage)
coef(summary(fit))
### 7.8.2 Splines ###
# regression spline
library(splines)
fit <- lm(wage~bs(age, knots=c(25, 40, 60)), data=Wage)
pred <- predict(fit, newdata=list(age=age.grid), se=T)
plot(age, wage, col='gray')
lines(age.grid, pred$fit, lwd=2)
lines(age.grid, pred$fit + 2 * pred$se, lty='dashed')
lines(age.grid, pred$fit - 2 * pred$se, lty='dashed')
# produce a spline with knots at uniform quantiles
dim(bs(age, knots=c(25, 40, 60)))
dim(bs(age, df=6))
attr(bs(age, df=6), 'knots')
# natural spline with four degrees of freedom
fit2 <- lm(wage~ns(age, df=4), data=Wage)
pred2 <- predict(fit2, newdata=list(age=age.grid), se=T)
lines(age.grid, pred2$fit, col='red', lwd=2)
# smoothing spline
plot(age, wage, xlim=agelims, cex=.5, col='darkgrey')
title('Smoothing Spline')
fit <- smooth.spline(age, wage, df=16)
fit2 <- smooth.spline(age, wage, cv=TRUE)
fit2$df
lines(fit, col='red', lwd=2)
lines(fit2, col='blue', lwd=2)
legend('topright', legend=c('16 DF', '6.8 DF'), col=c('red', 'blue'), lty=1, lwd=2, cex=.8)
# local regression
plot(age, waga, xlim=agelims, cex=.5, col='darkgrey')
title('Local Regression')
# local regression
plot.new()
plot(age, waga, xlim=agelims, cex=.5, col='darkgrey')
plot(age, wage, xlim=agelims, cex=.5, col='darkgrey')
title('Local Regression')
fit <- loess(wage~age, span=.2, data=Wage)
fit2 <- loess(wage~age, span=.5, data=Wage)
lines(age.grid, predict(fit, data.frame(age=age.grid), col='red', lwd=2))
lines(age.grid, predict(fit2, data.frame(age=age.grid), col='blue', lwd=2))
legend('topright', legend=c('Span=0.2', 'Span=0.5'), col=c('red', 'blue'), lty=1, lwd=2, cex=.8)
lines(age.grid, predict(fit, data.frame(age=age.grid)), col='red', lwd=2)
lines(age.grid, predict(fit2, data.frame(age=age.grid)), col='blue', lwd=2)
### 7.8.3. GAMs ###
# GAM by lm() function
gam1 <- lm(wage~ns(year, 4)+ns(age,5)+education, data=Wage)
# GAM by gam() function
library(gam)
install.packages("gam")
# GAM by gam() function
library(gam)
gam.m3 <- gam(wage~s(year, 4),+s(age, 5)+education, data=Wage)
gam.m3 <- gam(wage~s(year, 4)+s(age, 5)+education, data=Wage)
# plot
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE, col='blue')
# plot lm object
plot.gam(gam1, se=TRUE, col='red')
# plot lm object
plot.Gam(gam1, se=TRUE, col='red')
# ANOVA tests to determine the best model
gam.m1 <- gam(wage~s(age, 5)+education, data=Wage
# ANOVA tests to determine the best model
gam.m1 <- gam(wage~s(age, 5)+education, data=Wage)
# ANOVA tests to determine the best model
gam.m1 <- gam(wage~s(age, 5)+education, data=Wage)
gam.m2 <- gam(wage~year+s(age, 5)+education, data=Wage)
anova(gam.m1, gam.m2, gam.m3, test='F')
# summary
summary(gam.m3)
# prediction
preds <- predict(gam.m2, newdata=Wage)
# use local regression fits
gam.lo <- gam(wage~s(year, df=4)+lo(age, span=0.7)+education, data=Wage)
plot.gam(gam.lo, se=TRUE, col='green')
plot.Gam(gam.lo, se=TRUE, col='green')
# create interactions
gam.lo.i <- gam(wage~lo(year, age, span=0.5)+education, data=Wage)
# plot
library(akima)
install.packages("akima")
# plot
library(akima)
plot(gam.lo.i)
# logistic regression GAM
gam.lr <- gam(I(wage > 250)~year+s(age, df=5)+education, family=binomial, data=Wage)
par(mfrow=c(1, 3))
plot(gam.lr, se=T, col='green')
table(education, I(wage > 250))
gam.lr.s <- gam(I(wage > 250)~year+s(age, df=5)+education, family=binomial, data=Wage, subset=(education!='1. < HS Grad'))
plot(gam.lr.s, se=T, col='green')
set.seed(1)
sample(10)
# preparation
set.seed(1)
sample(10)
# preparation
RNGkind(sample.kind = "Rounding")
set.seed(1)
sample(10)
# Question 1
library(ISLR)
attach(Wage)
q1.1 <- lm(wage~education+age, data=Wage)
q1.2 <- lm(wage~education+poly(age, 2), data=Wage)
q1.3 <- lm(wage~education+poly(age, 3), data=Wage)
q1.1
q1.2
q1.2
q1.3
summary(q1.3)
anova(q1.1, q1.2, q1.3)
# Question 2
coef(summary(q1.3))
# Question 3
cut(age, breaks=c(0, 25, 35, 45, 55, 80))
q3 <- lm(wage~cut(age, breaks=c(0, 25, 35, 45, 55, 80), data=Wage)
q3 <- lm(wage~cut(age, breaks=c(0, 25, 35, 45, 55, 80), data=Wage))
q3 <- lm(wage~cut(age, breaks=c(0, 25, 35, 45, 55, 80)), data=Wage)
coef(summary(q3))
predict(q3, newdata=35)
predict(q3, newdata=c(35))
predict(q3, newdata=list(age=35))
predict(q3, newdata=list(age=35), se=T)
# Question 4
library(splines)
q4.1 <- lm(wage~bs(age, knots=c(25, 40, 60)), data=Wage)
q4.2 <- lm(wage~ns(age, df=4), data=Wage)
q4.3 <- smooth.spline(age, wage, df=16)
q4.4 <- loess(wage~age, span=.5, data=Wage)
age.grid <- seq(from=agelims[1], to=agelims[2])
par(mfrow=c(1,1))
plot.new()
plot(age, wage, xlim=agelims, cex=.5, col='darkgrey')
lines(age.grid, predict(q4.1, data.frame(age=age.grid)), col='red', lwd=2)
lines(age.grid, predict(q4.2, data.frame(age=age.grid)), col='blue', lwd=2)
lines(age.grid, predict(q4.3, data.frame(age=age.grid)), col='green', lwd=2)
lines(q4.3, col='green', lwd=2)
lines(age.grid, predict(q4.4, data.frame(age=age.grid)), col='orange', lwd=2)
predict(q4.1, newdata=list(age=55))
predict(q4.2, newdata=list(age=55)) # 118.2185
predict(q4.4, newdata=list(age=55)) # 118.406
predict(q4.3, newdata=list(age=55)) # 118.406
q4.3
summary(q4.3)
predict(q4.3)
predict(q4.3)$y
predict(q4.3)$y[1]
predict(q4.3)$y[55-18]
predict(q4.3)$y[55-18+1]
# Question 5
library(gam)
q5 <- gam(wage~s(age, 5)+year+education, data=Wage)
coef(q5)
coef(summary(q5))
coef(q5)
# Question 6
wage
# Question 6
education
# Question 6
predict(q5, newdata=data.frame(age=48, year=2008, education='5. Advanced Degree'))
# Question 7
q7 <- gam(wage~s(age, 3)+year+education, data=Wage)
predict(q7, newdata=data.frame(age=48, year=2008, education='5. Advanced Degree')) # 156.9727
setwd("~/Documents/UCLA_Extension/Week_4")
setwd("~/Documents/UCLA_Extension/")
